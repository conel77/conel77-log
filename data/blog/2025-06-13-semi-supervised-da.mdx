---
title: "Semi Supervised Domain Adaptationê³¼ Noisy Label Learning"
date: "2025-06-13"
tags: ['domain adaptation', 'semi-supervised', 'noisy label', 'protonet']
draft: false
summary: "Noisy label ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê´€ì ì—ì„œ Semi-Supervised Domain Adaptation(SSDA) ë°©ì‹ì— ëŒ€í•œ ì„¤ëª…"
---

# Semi-Supervised Domain Adaptation with Source Label Adaptation

Semi Supervised Domain Adaptation ì˜ ì „ì œì¡°ê±´ : ë³´ì§€ ëª»í•œ target dataì™€ ëª‡ì¥ì˜ ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë‹¤ëŸ‰ì˜ target domain ê³¼ ë¹„ìŠ·í•œ ê²°ì˜ source dataë¥¼ ì‚¬ìš©í•´ì„œ classification í•˜ëŠ” ì‘ì—…

ê¸°ì¡´ì˜ ì‘ì—…ë“¤ì€ ì†ŒìŠ¤ ë°ì´í„°ì— noise ê°€ ê»´ì„œ label ì´ ì˜ëª»ë˜ì–´ìˆê±°ë‚˜, target dataì™€ match ê°€ ì•ˆëœë‹¤ëŠ” ë¬¸ì œì ì„ ì§€ë‹ˆê³  ìˆìŒ â†’ ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” source dataë¥¼ target data ë¡œ adaptí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆ

ë˜í•œ target data ê´€ì ì—ì„œ noisy í•œ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨, íƒ€ê²Ÿ ë°ì´í„°ì— ì˜ ë§ìœ¼ë©´ì„œ ì˜ëª»ëœ ë¼ë²¨ë§ì„ ê°€ë ¤ë‚¼ ìˆ˜ ìˆëŠ” ë°©ì‹ ë˜í•œ ì œì•ˆí•¨ 

---

## Method

ê¸°ì¡´ semi supervised ë°©ì‹ì€ ë‹¨ìˆœíˆ ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„°ê°„ cross entropy loss ë¥¼ ê±¸ì–´ì„œ domain adaptation ì„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜, ì´ ë°©ì‹ê°™ì€ ê²½ìš°ì—ëŠ” domain shift ë¬¸ì œê°€ ë°œìƒí•œë‹¤.(íƒ€ê²Ÿê³¼ ì†ŒìŠ¤ê°„ ì°¨ì´ê°€ ë„ˆë¬´ ì»¤ì„œ adaptation ì´ ì˜ ì•ˆë˜ëŠ” ê²ƒ)

ë˜ ì´ ë¬¸ì œ í•´ê²°í•˜ê¸° ìœ„í•´ ! ë¼ë²¨ë§ ì•ˆëœ ë°ì´í„°ì…‹ì—ë‹¤ê°€ ìˆ˜ë„ ë¼ë²¨ë§ì„ í•˜ê±°ë‚˜, entropy minimization ì„ ì§„í–‰í•˜ê±°ë‚˜, comsistency regularization ë“±ì„ ì§„í–‰í•œë‹¤.

â†’ ê·¼ë° ì´ëŸ°ê²ƒë„ íƒ€ê²Ÿ ë°ì´í„°ê°€ ì–´ëŠì •ë„ ì†ŒìŠ¤ ë°ì´í„°ì™€ semanticí•˜ê²Œ ë¹„ìŠ·í•´ì•¼í•œë‹¤ëŠ” ê²ƒì„ ì „ì œí•˜ë¯€ë¡œ, í•œê³„ê°€ ëª…í™•í•˜ê²Œ ì¡´ì¬í•œë‹¤. 

ë‹¨ìˆœíˆ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œë§Œ ì ‘ê·¼ì„ í•˜ë©´ domain adaptation ë‹¹ì‹œ target ë°ì´í„°ê°€ ì˜ëª»ëœ source label ê³¼ ë¹„ìŠ·í•´ì§ˆ ì—¼ë ¤ê°€ ìˆë‹¤. 

ê·¸ë‹ˆê¹Œ íƒ€ê²Ÿ ë°ì´í„° ì…ì¥ì—ì„œëŠ” ë…¸ì´ì¦ˆê°€ ë‚€ ì†ŒìŠ¤ ë°ì´í„°(= ideal label)ê°€ ë  ìˆ˜ ìˆë‹¤ëŠ” ì (ì‹¤ì œë¡œ ë…¸ì´ì¦ˆê°€ ë‚€ê²Œ ì•„ë‹ˆë¼, target ì— ì˜ ë§ì§€ ì•ŠëŠ” source dataë¼ëŠ” ëœ»)

**ì´ì— ëŒ€í•œ ëª…ì¹­ì„ Nosiy label learning** ì´ë¼ê³  ë¶€ë¦„ 

â†’ ë³¸ ë…¼ë¬¸ì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì–¼ë§Œí¼ ì˜ ideal í•œ target spaceì— ë§ê²Œ adapted label ë¡œ ë§ì¶”ë„ë¡ í•™ìŠµí•  ìˆ˜ ìˆëŠëƒê°€ ë˜ëŠ” ê²ƒì´ë‹¤.

---

NLL ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ê³  í•˜ëŠ” ë‹¤ì–‘í•œ ë…¼ë¬¸ë“¤ì´ ìˆì—ˆëŠ”ë°, ì´ëŸ° ë°©ë²•ë“¤ì´ ì§ì ‘ì ìœ¼ë¡œ SSDAë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì˜€ê¸° ë•Œë¬¸ì—, ì´ ë‘˜ì˜ ë„¤íŠ¸ì›Œí¬ ê°„ ì—°ê²°ì„±ì„ í’€ê³ ì í•˜ì˜€ë‹¤.

ì¼ë‹¨ noisy í•œ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ideal í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œ, DAë°©ì‹ì„ NLL ì²˜ëŸ¼ í•´ê²°í•˜ë ¤ê³  í–ˆëŠ”ë°,

1. ê¸°ì¡´ì— noisy ê°€ ë‚€ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ideal í•œ noisy ë‚€ ì†ŒìŠ¤ ë°ì´í„°ë¡œ ë³´ê³  í•œë²ˆ ê±°ë¥´ëŠ” ì‘ì—…ì„ ì§„í–‰
2. ê·¼ë° ì´ëŸ°ì‹ìœ¼ë¡œ ê±°ë¥´ë©´ ê²°êµ­ì— ê±¸ëŸ¬ì§„ ë°ì´í„°ê°€ noisy ë‚€ ë°ì´í„° (ê¸°ì¡´) ê²ƒê³¼ ë¹„ìŠ·í•´ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤. source dataì¸ ìê¸°ìì‹ ë§Œ ë³´ê³  updateë¥¼ í•˜ê¸° ë•Œë¬¸ì— (ì´ê²Œ label correction with self-predictionì´ë¼ê³  í•¨)
3. ë”°ë¼ì„œ target domain ê´€ì ì—ì„œ ideal label ì„ ë§Œë“œëŠ” ì‘ì—…ì„ ì±„íƒí•˜ì˜€ë‹¤. ($g_c()$ ê°€  ë°”ë¡œ ê·¸ ëª¨ë¸ì„)

![image.png](/workspace/conel77-log/public/static/images/semi-supervised-da/image.png)

ê·¸ë ‡ë‹¤ë©´ ì´ ë¼ë²¨ì˜ í™•ë¥ ë¶„í¬ ê²°ê³¼ë¥¼ ë‚´ë±‰ëŠ” ëª¨ë¸ $g_c()$ ë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì •ì˜í•  ê²ƒì¸ê°€?

### Protonet with Pseudo Centers

ë°”ë¡œ ì´ Protonetì„ ì‚¬ìš©í•˜ëŠ”ë°, 

ì´ ëª¨ë¸ì€ Protonet with Pseudo Centers ë¥¼ ì‚¬ìš©í•˜ì—¬ few shot labeling ëœ íƒ€ê²Ÿ ì´ë¯¸ì§€ê°€ ìˆì—ˆì„ ë•Œ overfitting ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•˜ì˜€ë‹¤. 

![image.png](/workspace/conel77-log/public/static/images/semi-supervised-da/image 1.png)

**$c_k$ ëŠ” Target domainì˜ unlabeled data**ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ í´ë˜ìŠ¤ë³„ ì¤‘ì‹¬(Prototype)

ì½”ë“œë¡œ í™•ì¸í•´ë³´ë©´, 

```python
t_pred, t_feat = prediction(t_loader, model)
label = t_pred.argmax(dim=1)
center = [t_feat[label == i].mean(dim=0) for i in range(num_classes)]
```

- ì—¬ê¸°ì„œ `t_loader`ëŠ” **target domainì˜ validation or unlabeled test loader**
- ë”°ë¼ì„œ `self.center`ëŠ” **target domain feature space** ê¸°ì¤€ì˜ ì¤‘ì‹¬

![image.png](/workspace/conel77-log/public/static/images/semi-supervised-da/image 2.png)

ì…ë ¥ `x`ëŠ” ì¼ë°˜ì ìœ¼ë¡œ **source domainì˜ feature** (í•™ìŠµìš© ë°ì´í„°)ë¡œ,  ì´ `x`ì™€ **target ë„ë©”ì¸ì˜ ì¤‘ì‹¬ (`self.center`) ê°„ ê±°ë¦¬**ë¥¼ ë°”íƒ•ìœ¼ë¡œ soft label ìƒì„±í•œë‹¤.

```python
dist = torch.cdist(x, self.center)
soft_label = F.softmax(-dist * T, dim=1)
```

ì´ soft labelì€ "ì´ source featureê°€ target domain ìƒì—ì„œ ì–´ë–¤ class prototypeì— ê°€ê¹Œìš´ê°€?"

ë¼ëŠ” **target-aligned ì˜ˆì¸¡ í™•ë¥ **ì„ ë‚˜íƒ€ë‚¸ë‹¤. 

ì¦‰, self.centerëŠ” target feature ê³µê°„ì—ì„œ ê° í´ë˜ìŠ¤ì˜ í‰ê·  ë²¡í„° (prototype)ê°€ ë˜ëŠ”ê±°ê³ , soft label(prototype network ì˜ output) ì€ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ê°€ì§€ëŠ” ê²ƒ!

![image.png](/workspace/conel77-log/public/static/images/semi-supervised-da/image 3.png)

ìµœì¢… ì†ŒìŠ¤ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ ë˜ëŠ” ë¼ë²¨ ì‹ì„ ë³´ë©´, ì•ŒíŒŒ ê°’ì„ í†µí•´ hard label(ì›í•«ì¸ì½”ë”©) ê³¼ soft label(í™•ë¥ ë¶„í¬)ê°„ì˜ ë¹„ìœ¨ì„ ì¡°ì •í•´ì£¼ê³ , prototype network ëŠ” unlabeled ëœ target data ë¥¼ self.centerë¡œ ë‘ì–´ source data ê°€ ë“¤ì–´ê°”ì„ ë•Œ ë±‰ëŠ” soft label ì´ë¼ê³  ë³´ë©´ ëœë‹¤.

| íŒŒë¼ë¯¸í„° | ì—­í•  | ê°’ | í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ |
| --- | --- | --- | --- |
| `--alpha` | SLA ì†ì‹¤ì—ì„œ hard labelê³¼ soft labelì˜ ë¹„ìœ¨ | 0.3 | hard label:soft label = 0.7:0.3 â†’ soft labelì€ ë³´ì¡°ì ì¸ íŒíŠ¸ ì—­í•  |
| `--update_interval` | prototype(center) ì—…ë°ì´íŠ¸ ì£¼ê¸° | 500 | ëª¨ë¸ì´ ë³€í™”í•¨ì— ë”°ë¼ target prototypeì„ ì–¼ë§ˆë‚˜ ìì£¼ ìƒˆë¡œ ê³„ì‚°í• ì§€ |
| `--warmup` | SLA ì‹œì‘ ì‹œì  | 50000 | 5ë§Œ ìŠ¤í… ì „ê¹Œì§€ëŠ” ì¼ë°˜ CE lossë§Œ ì‚¬ìš© â†’ ì•ˆì •í™”ëœ í›„ soft label ì‚¬ìš© ì‹œì‘ |
| `--T` | prototype soft labelì˜ temperature | 0.6 | ë‚®ì„ìˆ˜ë¡ ë” "í™•ì‹  ìˆëŠ”" soft label â†’ sharpened ë¶„í¬ë¡œ í•™ìŠµ ê°•ì œí•¨ |

---

## MME+ SLA

SLA ê°™ì€ ê²½ìš°ì—ëŠ” ëª¨ë“ˆë¡œì¨ ë„¤íŠ¸ì›Œí¬ í†µìœ¼ë¡œ ì œì•ˆí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ê¸°ì¡´ SOTA ëª¨ë¸ì— ë¶™ì—¬ì„œ ì‚¬ìš©í–ˆì—ˆì„ ë•Œ adaptation ì„±ëŠ¥ì´ ì˜¬ë¼ê°„ë‹¤ëŠ” ê²ƒì„ ì œì•ˆí•˜ì˜€ëŠ”ë°, ì´ SOTAëª¨ë¸ë¡œ MMEì™€ CDACë¥¼ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.

MMEëŠ”**Minimum Entropy for Semi-supervised Domain Adaptation**ì˜ ì•½ìë¡œ, ë„ë©”ì¸ ì ì‘ì—ì„œ **target ë„ë©”ì¸ì— labelì´ ì—†ì„ ë•Œ**ì—ë„ **ì˜ë¯¸ ìˆëŠ” í”¼ë“œë°±ì„ ì£¼ëŠ”** ê¸°ë²•ì´ë‹¤. (ì¦‰ unlabled targetì— ëŒ€í•œ loss ë¥¼ ì œì•ˆí•œ ë…¼ë¬¸ì´ì£ )

ì´ ê°œë…ì€ 2020ë…„ CVPR ë…¼ë¬¸ì¸ **"Minimum Class Confusion for Domain Adaptation"** ë° **"Unsupervised Domain Adaptation by Minimizing Conditional Entropy"** ë“±ì˜ ì˜í–¥ì„ ë°›ì•˜ë‹¤.

ê¸°ì¡´ domain adaptation ì—ì„œëŠ” source domainì—ëŠ” labelì´ ìˆì§€ë§Œ, **target domainì—ëŠ” labelì´ ì—†ë‹¤ëŠ” ì „ì œ ì¡°ê±´ê³¼,** ì¼ë°˜ì ìœ¼ë¡œ target ìƒ˜í”Œì€ loss ê³„ì‚°ì— ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë–„ë¬¸ì—, í•™ìŠµì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ 2020ë…„ë„ì—ëŠ” ì¡´ì¬í–ˆì—ˆë‹¤. MME ëŠ” ì´ì— ëŒ€í•´ target domain ë°ì´í„°ì— ëŒ€í•´ì„œë„ low entropy ì˜ˆì¸¡ì„ í•˜ë„ë¡ ëª¨ë¸ì„ êµ¬ì„±í•˜ìëŠ”ê²Œ ë©”ì¸ìœ¼ë¡œ, MME Lossë¥¼ 

MMEëŠ” target ì…ë ¥ $x_t$ ì— ëŒ€í•´ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµëœë‹¤.

$\mathcal{L}_{\text{MME}} = \mathbb{E}_{x_t \sim \mathcal{D}_t} \left[ \sum_{c} p_c(x_t) \log p_c(x_t) \right]$

```python
def mme_loss(self, _, x, lamda=0.1):
out = self.forward(x, reverse=True)           # reverse = gradient reversal
out = F.softmax(out, dim=1)                   # í™•ë¥  ë¶„í¬í™”
return lamda * torch.mean(torch.sum(out * (torch.log(out + 1e-10)), dim=1))
```

out: target sampleì— ëŒ€í•œ softmax ê²°ê³¼
torch.sum(out * log(out)): entropy
lamda: lossì— ì ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ 

ì´ëŸ°ì‹ìœ¼ë¡œ lossë¥¼ êµ¬ì„±í•˜ê²Œ ë˜ë©´ unlabledëœ target dataì—ë„ labeling ì´ í•˜ë‚˜ì— í´ë˜ìŠ¤ì— confident í•˜ê²Œ í™•ë¥  ë¶„í¬ê°€ êµ¬ì„±ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë˜ê²Œ ë˜ë¯€ë¡œ, íƒ€ê²Ÿ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ì¤„ì´ë„ë¡ í•™ìŠµë˜ëŠ” ë°©ì‹ì´ë¼ê³  ë³´ë©´ ëœë‹¤.

ğŸ§  ì°¸ê³ :

reverse=True â†’ Gradient Reversal Layer ì‚¬ìš© (DANNê³¼ ë™ì¼í•œ ê¸°ë²•)

Gradient Reversal Layer(GRL)ëŠ” ë„ë©”ì¸ ì ì‘ì—ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜í•œ ì—°ì‚°ìœ¼ë¡œ, ìˆœì „íŒŒì—ì„œëŠ” ì…ë ¥ì„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ì§€ë§Œ ì—­ì „íŒŒ ì‹œì—ëŠ” gradientì˜ ë¶€í˜¸ë¥¼ ë°˜ì „ì‹œì¼œ ëª¨ë¸ì´ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ ì†ì´ë„ë¡ í•™ìŠµí•˜ê²Œ ë§Œë“ ë‹¤. ì´ë¥¼ í†µí•´ feature extractorê°€ sourceì™€ target ë„ë©”ì¸ì„ êµ¬ë¶„í•  ìˆ˜ ì—†ë„ë¡ ì¼ë°˜í™”ëœ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê²Œ í•˜ë©°, adversarial í•™ìŠµ êµ¬ì¡°ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.

ì¦‰, **target ì˜ˆì¸¡ì˜ entropyë¥¼ ì¤„ì´ëŠ” ê²ƒ** â†’ **ê²°ê³¼ì ìœ¼ë¡œ ë” confidentí•œ ê²°ì • ê²½ê³„ë¥¼ ìœ ë„**

---

## CDAC + SLA

CDACëŠ” "Class-aware Domain Alignment with Consistency for Semi-supervised Domain Adaptation"ì˜ ì•½ìë¡œ, **MMEì˜ entropy minimization** ê°œë…ì„ í™•ì¥í•˜ì—¬, í´ë˜ìŠ¤ ì¤‘ì‹¬(class-aware)ê³¼ ì¼ê´€ì„±(consistency)ê¹Œì§€ í†µí•©í•œ ë°©ì‹ì´ë‹¤.

CDACëŠ” í¬ê²Œ 3ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ë¡œ êµ¬ì„±ëœë‹¤ : 

| êµ¬ì„± ìš”ì†Œ | ì†ì‹¤ í•¨ìˆ˜ | ëª©ì  |
| --- | --- | --- |
| â‘  Adversarial Alignment | `advbce_unlabeled` | ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ ìœ ë„ |
| â‘¡ Pseudo Label Supervision | `pl_loss` | confident pseudo-label ì˜ˆì¸¡ì„ ì •ë‹µì²˜ëŸ¼ í•™ìŠµ |
| â‘¢ Consistency Regularization | `con_loss` | ê°™ì€ ì…ë ¥ì˜ augmentation ê°„ ì˜ˆì¸¡ ì¼ê´€ì„± ìœ ì§€ |

## CDAC ì½”ë“œ ë¶„ì„: `cdac_loss`

```python
def cdac_loss(self, step, x, x1, x2):
    w_cons = 30 * sigmoid_rampup(step, 2000)  # consistency ì ì§„ì  ì¦ê°€
```

- `x`: unlabeled target ì›ë³¸
- `x1`, `x2`: ë™ì¼í•œ `x`ì— ëŒ€í•œ augmentation (ex. strong, weak)
- `step`: í˜„ì¬ iteration (ì¼ì • step í›„ì— consistencyë¥¼ ì ì°¨ ì¦ê°€)

ì„¸ê°€ì§€ ë¡œìŠ¤ë¥¼ ê°ê° ëœ¯ì–´ë³´ë©´,

### â‘  Adversarial Alignment Loss (`aac_loss`)

```python
f = self.f(x)
f1 = self.f(x1)
out = self.c(f, reverse=True)
out1 = self.c(f1, reverse=True)

prob, prob1 = F.softmax(out, dim=1), F.softmax(out1, dim=1)
aac_loss = advbce_unlabeled(
    target=None, f=f, prob=prob, prob1=prob1, bce=self.bce
)
```

```python
def advbce_unlabeled(target, f, prob, prob1, bce):
"""
Construct adversarial adaptive clustering loss.
Args:
target: (optional, unused here) ground-truth label or pseudo-label
f: feature vectors from original input x (shape: [B, D])
prob: softmax outputs of x (shape: [B, C])
prob1: softmax outputs of x1 (augmented) (shape: [B, C])
bce: a binary cross-entropy-like loss module (BCE_softlabels)
"""
target_ulb = pairwise_target(f, target)                     # shape: [B * B]
prob_bottleneck_row, _ = PairEnum2D(prob)                   # shape: [B * B, C]
_, prob_bottleneck_col = PairEnum2D(prob1)                  # shape: [B * B, C]

adv_bce_loss = -bce(prob_bottleneck_row, prob_bottleneck_col, target_ulb)
return adv_bce_loss
```

- `reverse=True`: Gradient Reversal Layer (GRL)ë¥¼ ê±°ì³ì„œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ ì†ì´ë„ë¡ í•™ìŠµ
- `advbce_unlabeled`: ë„ë©”ì¸ ë¶ˆë³€ feature ìœ ë„ë¥¼ ìœ„í•œ adversarial binary cross entropy ì†ì‹¤

â†’ **GRL + Adversarial BCE** = MMEì²˜ëŸ¼ ì‘ë™, ê·¸ëŸ¬ë‚˜ ë‘ ê°€ì§€ ì…ë ¥(f, f1)ìœ¼ë¡œ ì •ë°€í™”

---

### â‘¡ Pseudo Label Loss (`pl_loss`)

```python
out = self.c(f)     # xì˜ ì¼ë°˜ ë¶„ë¥˜ ì˜ˆì¸¡
out2 = self.c(self.f(x2))  # x2 = augmented x

prob = F.softmax(out, dim=1)
mp, pl = torch.max(prob.detach(), dim=1)
mask = mp.ge(0.95).float()  # confidenceê°€ 95% ì´ìƒì¼ ë•Œë§Œ ì‚¬ìš©

pl_loss = (F.cross_entropy(out2, pl, reduction="none") * mask).mean()

```

- `pl`: confidentí•œ pseudo-label
- `mask`: í™•ì‹ ë„ê°€ ë†’ì€ ìƒ˜í”Œë§Œ ì†ì‹¤ì— í¬í•¨
- `out2`: augmented viewì˜ ì˜ˆì¸¡ì´ pseudo-labelê³¼ ë§ë„ë¡ ìœ ë„

â†’ **Self-trainingê³¼ ìœ ì‚¬**, confidentí•œ ì˜ˆì¸¡ì€ ì •ë‹µì²˜ëŸ¼ í•™ìŠµ

---

### â‘¢ Consistency Loss (`con_loss`)

```python
con_loss = F.mse_loss(prob1, prob2)
```

- `prob1`: `x1`ì— ëŒ€í•œ softmax
- `prob2`: `x2`ì— ëŒ€í•œ softmax
- ê°™ì€ ìƒ˜í”Œ(x)ì˜ ì„œë¡œ ë‹¤ë¥¸ augmentation ê°„ì— **ì˜ˆì¸¡ ì¼ê´€ì„±**ì„ ìœ ì§€í•˜ë„ë¡ ê°•ì œ

---

## ğŸ“ ìµœì¢… CDAC ì†ì‹¤ í•¨ìˆ˜

```python
return aac_loss + pl_loss + w_cons * con_loss
```

- ì„¸ ê°€ì§€ ì†ì‹¤ì„ ë”í•¨
- `w_cons`: consistency lossëŠ” **ì´ˆê¸°ì—ëŠ” ë¬´ì‹œ**í•˜ê³ , **í•™ìŠµì´ ì•ˆì •ë˜ë©´ ì ì°¨ ì¦ê°€**

ì´ë ‡ê²Œ ê°ê° MME, CDACì—ì„œ ì œì•ˆí•œ Lossì— SLA lossë¥¼ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í•œ ê²ƒì´ë‹¤.

---

## ğŸ” 1. Hard Label vs. Soft Label

| êµ¬ë¶„ | ì •ì˜ | ì˜ˆì‹œ (C=3) | ì„¤ëª… |
| --- | --- | --- | --- |
| **Hard Label** | ì •ë‹µ í´ë˜ìŠ¤ë§Œ 1, ë‚˜ë¨¸ì§€ëŠ” 0 | `[0, 1, 0]` | í´ë˜ìŠ¤ 1ì´ ì •ë‹µ |
| **Soft Label** | ëª¨ë¸ ì¶œë ¥ì˜ í™•ë¥  ë¶„í¬ | `[0.2, 0.7, 0.1]` | ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ |

---

## ğŸ” 2. ì™œ "soft-label"ì„ ì“°ëŠ”ê°€?

íŠ¹íˆ **semi-supervised learning**, **domain adaptation**, **knowledge distillation**, **SLA (Soft Label Adaptation)** ê°™ì€ ìƒí™©ì—ì„œëŠ”:

- **Unlabeled ë°ì´í„°**ì— ëŒ€í•´ "ì •ë‹µ"ì´ ì—†ìŒ
- ë”°ë¼ì„œ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì˜ˆì¸¡ê°’ì„ ì‹ ë¢°ë„ ìˆê²Œ ì¶”ì •í•˜ì—¬ **pseudo-label**ë¡œ ì‚¬ìš©
- ì´ë•Œ **í™•ë¥  ë¶„í¬ ì „ì²´ë¥¼ í™œìš©**í•˜ëŠ” ê²ƒì´ hard labelë³´ë‹¤ ë” ìœ ì—°í•˜ê³  ì •ë³´ëŸ‰ì´ ë§ìŒ

```python
logits = model(x)            # shape: (batch, num_classes)
soft_label = F.softmax(logits / T, dim=1)  # softmax with temperature

```

- `T` (temperature) â†‘ë©´ ë¶„í¬ê°€ ë” flat (ë¶€ë“œëŸ½ê³ , ë¶ˆí™•ì‹¤ì„± ë°˜ì˜)
- ì´ `soft_label`ì€ cross-entropyì—ì„œ targetìœ¼ë¡œ ì“°ì¼ ìˆ˜ ìˆìŒ:

```python
loss = F.kl_div(F.log_softmax(pred, dim=1), soft_label, reduction='batchmean')
```

---
