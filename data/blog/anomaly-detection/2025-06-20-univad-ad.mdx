---
title: "UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly Detection(CVPR2025)"
date: "2025-06-20"
tags: ['anomaly detection', 'unified model', 'few-shot']
draft: false
summary: "다양한 도메인의 데이터 환경에서 unified model 을 통한 anomaly detection을 제안한 논문 리뷰"
---

# UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly Detection

기존 Visual anomaly detection의 문제점은 특정 도메인에만 너무 치우져져 있다는 점 → custom algorithm이 만들어지기 때문에 다른 도메인으로 옮겨서 detection을 수행하면 성능이 잘 나오지 않는다.

따라서 training free 의 domain이 Unified 된 단일 모델을 제안하였다. 

![problem.png](/static/images/univad-ad/problem.png)

Anomaly Detection 분야의 SOTA모델들만 봐도, 특정 Domain dataset에만 성능이 잘 나온다는 점 - 이러한 모델들에 다른 domain 의 dataset을 넣게되면 성능이 크게 떨어진다는 점부터,
Class별로 학습을 각각 시켜줘야한다는 점까지 불편한 점이 많았다. 따라서 다양한 domain 의 data들을 넣어도, 이상 탐지를 굉장히 잘 할 수 있는 모델 구축으로 trend가 진행되고 있는 것 같다.

---

## Contextual Component Clustering

**Contextual Component Clustering (C3)** 모듈은

적은 수의 normal 이미지(few-shot)에서도 정확한 component segmentation을 하기 위해 pretrained 된 모델에 clustering 방식을 조합하는 모듈을 제안하였다.

---

### 1. Recognize Anything + Grounded SAM

**단계 1: 객체 인식**

- 입력 이미지 → **Recognize Anything Model (RAM)** 사용
    
    이미지 안의 객체를 텍스트 태그(content tags)로 자동 인식 (예: ‘wheel’, ‘pipe’, ‘surface’ 등)
    

**단계 2: 세그멘테이션 마스크 생성**

- **Grounded SAM**을 사용하여 텍스트 태그에 따라 **component 마스크들**을 생성
    
    → 이때 생성된 마스크 집합을 $$M_{\text{sam}} ∈ ℝ^{M×H×W}$$로 표현
    
    (M: 감지된 마스크 수)
    

---

### 2. SAM 마스크 후처리 (정제 및 필터링)

SAM 기반 마스크들은 너무 조각나거나(세분화), 너무 뭉뚱그려질 수 있어서 아래와 같은 후처리를 진행한다.

**(1) 마스크가 단 하나뿐이고, 거의 전체 이미지를 덮음 (exceed $γ%$)**

→ 이는 **질감 표면 이미지(texture surface)** (예: 나무 판, 천 표면 등)로 간주해서 전체 이미지를 하나의 요소로 취급한다.


$$M = 1ᴴ×ᵂ$$ (전체가 1인 마스크 하나)

**(2) 마스크가 단 하나인데 이미지의 작은 부분만 차지 (under $γ%$)**

→ 단일 객체가 있을 가능성이 있기 때문에, 이 마스크를 그대로 사용한다. 


$$M = M_sam$$


---

### 3. 마스크가 여러개인 경우: 클러스터링 진행

이제부터가 본격적인 **Contextual Clustering** 부분인데, SAM이 여러 마스크를 반환할 경우에, 이를 클러스터링해서 rearrange를 시켜야한다. 

![clustering.png](/static/images/univad-ad/clustering.png)

먼저, 정상 이미지들(K장)에 대해 사전 학습된 이미지 인코더(예: CLIP ViT 등)를 통해 **공간 해상도를 유지한 채 feature map $(Fₙ ∈ ℝᴷ×ᴴ₁×ᵂ₁×C)$** 을 추출한다. 이 feature map은 각 위치(픽셀 단위)가 C차원의 의미적 표현을 가지는 구조로, 개별 위치가 갖는 시각적 의미를 표현한다고 볼 수 있다.

이후, 전체 정상 이미지의 feature map을 위치 단위로 펼쳐서 하나의 feature 벡터 집합으로 만든다. 구체적으로는 K × H₁ × W₁ 개의 feature 벡터를 크기 C의 벡터로 flatten 하여 클러스터링의 입력으로 사용한다. 이 벡터 집합에 대해 K-means 알고리즘을 수행함으로써, 각 컴포넌트의 의미적 중심 역할을 하는 **N개의 클러스터 중심($C ∈ ℝᴺ×C$)** 을 얻는다.

이 클러스터 중심을 기반으로, 각 정상 이미지와 쿼리 이미지의 feature map 상 모든 위치 벡터가 어떤 클러스터에 속하는지 유사도(예: cosine similarity)에 따라 결정하고, 그 결과를 이용해 **클러스터 마스크($M_{cluster} ∈ ℝᴺ×ᴴ₁×ᵂ₁$)** 를 생성한다. 이 마스크는 N개의 채널로 구성되며, 각 채널은 특정 클러스터에 속하는 위치들을 이진값(1 또는 0)으로 표시한다. 즉, 각 마스크는 “같은 의미를 공유하는 영역”을 하나의 컴포넌트로 표현한 것이라고 볼 수 있다. 

하지만 이렇게 생성된 마스크들 중에는 실제로 유효한 객체(컴포넌트)를 포함하지 않는 것도 존재할 수 있다. 예를 들어, 배경 영역에 해당하는 위치들이 클러스터링 결과 하나의 마스크로 묶인 경우가 그러하다. 이를 걸러내기 위해 UniVAD는 간단하면서도 효과적인 **배경 필터링**을 도입했다. 각 마스크에 대해 **네 모서리 위치의 값이 모두 0인지**를 확인함으로써, 그 마스크가 이미지의 주변부(=배경)에 아무런 내용을 포함하지 않는지를 판단한다. 네 모서리는 일반적으로 객체가 존재하지 않을 가능성이 높은 이미지의 외곽부이기 때문에, 이 영역들 모두가 0이라는 것은 해당 마스크가 실제 객체보다는 배경에 더 가까운 분포를 가진다는 것을 의미한다.

이러한 방식으로 배경 마스크를 제거한 후 남는 유효한 마스크 집합을 $M_{\text{valid}} ∈ ℝ^{N’ × H₁ × W₁}$로 정의하며, 이후 Grounded SAM의 마스크와 IoU 기반 매칭을 통해 최종적으로 통합된 component 마스크를 구축하게 된다.

## 간략하게 정리해보자

처음으로, 이미지 특징을 추출하는데, 

- **사전 학습된 이미지 인코더** (예: CLIP ViT 등)를 사용해 normal 이미지들에서 feature map을 추출한다.
    
    → **$$Fn ∈ ℝᴷ×ᴴ₁×ᵂ₁×C$$**
    
    (K: 정상 이미지 수, $H₁×W₁$: 인코더 출력 해상도)
    

그리고 Fn 내의 feature들을 클러스터링을 진행한다 → N개의 중심점이 생성된다.

```
C ∈ ℝᴺ×C
```

쿼리 이미지와 normal 이미지의 각 위치(feature)에 대해 각 클러스터 중심과의 유사도를 계산하여 **클러스터 마스크 생성한다.** 


$$M_{\text{cluster}} ∈ ℝᴺ×ᴴ₁×ᵂ₁$$


마지막으로 유효하지 않은 마스크 제거를 진행한다 (배경 필터링)

마스크가 진짜 객체인지 확인하려고, **네 모서리 픽셀값이 모두 0인지를** 체크한다. 

배경은 제외하고, 남은 유효 마스크는 다음과 같이 표시한다. 


$$M_{\text{valid}} ∈ ℝᴺ′×ᴴ×ᵂ (N′ ≤ N)$$

---

### 4. SAM 마스크와 클러스터 마스크 mapping

우선, Grounded SAM을 통해 얻은 마스크들은 객체 인식 기반의 마스크로, 텍스트 태그에 따라 분할되어 있지만, 이미지 간 일관성이 부족하거나 너무 세분화되는 등의 문제가 있다. 반면, 클러스터 기반 마스크는 정상 이미지들의 피처맵을 K-means 클러스터링하여 생성한 것으로, 정상 상태의 컴포넌트 분포를 의미적으로 정제한 마스크라고 할 수 있다. 이 두 종류의 마스크는 서로 생성 방식이 다르기 때문에, 직접 비교 분석을 위해서는 하나의 기준으로 통합할 필요가 있다.

이를 위해 UniVAD는 각 SAM 마스크에 대해 모든 클러스터 마스크와의 IoU를 계산하고, 그 중 가장 높은 IoU를 갖는 클러스터 마스크를 찾는다. 즉, SAM 마스크와 가장 많이 겹치는 클러스터 마스크가 해당 SAM 마스크의 대표 클러스터로 간주된다. 수식으로 표현하면, 각 SAM 마스크 $M_{\text{sam}}^{(i)}$에 대해 가장 유사한 클러스터 마스크 $M_{\text{valid}}^{(j)}$는 다음과 같이 할당된다:
$$
\text{Label}(M_{\text{sam}}^{(i)}) = \arg\max_j \text{IoU}(M_{\text{sam}}^{(i)}, M_{\text{valid}}^{(j)})
$$

이 과정을 통해 모든 SAM 마스크는 하나의 클러스터 인덱스에 대응되며, 이후 각 클러스터 인덱스 j에 대해 해당 클러스터에 할당된 SAM 마스크들을 모두 합집합(union)한다. 이렇게 해서 얻어진 마스크는 $$M^{(j)} = \bigcup_{\text{Label}(M_{\text{sam}}^{(i)}) = j} M_{\text{sam}}^{(i)}$$ 로 표현되며, 최종적으로 하나의 컴포넌트를 대표하는 통합된 마스크가 된다.

이러한 방식은 서로 다른 기준으로 생성된 마스크들을 하나의 일관된 컴포넌트 기반 구조로 정렬해주며, 이후 이상 감지 단계에서 쿼리 이미지와 정상 이미지 간의 정확한 비교를 가능하게 만든다. 특히, 이 과정을 통해 생성된 컴포넌트 마스크는 이후 구조적 이상(CAPM)이나 논리적 이상(GECM) 탐지에서 중요한 기준이 된다.

결과적으로 이 IoU 기반의 마스크 할당 방식은, 의미적으로 유사한 영역을 통합하고 배경이나 무관한 마스크를 걸러내며, 다양한 객체 구성과 복잡한 시각적 구조를 효과적으로 정규화하는 데 핵심적인 역할을 수행한다.

## 간략하게 정리해보자

**IoU 기반 매칭**

- 각 SAM 마스크 $M_i^{\text{sam}}$와 클러스터 마스크 $M_j^{\text{valid}}$ 간의 **IoU (교차-합 비율)** 계산
- 가장 높은 IoU를 갖는 클러스터 마스크에 SAM 마스크를 할당한다.
    
    
    $$Label(M_i^{\text{sam}}) = argmax_j IoU(M_i^{\text{sam}}, M_j^{\text{valid}})$$
    
    

**최종 마스크 생성**

- 각 클러스터 인덱스 j에 대해
    
    해당 label로 지정된 SAM 마스크들을 **합집합(union)** 해서 새로운 마스크를 생성한다.
    
    
    $$M^j = ⋃_{Label(M_i^{\text{sam}}) = j} M_i^{\text{sam}}$$
    
    

- 최종 component 마스크 집합:
    
    
    $$M ∈ ℝᴺ′×ᴴ×ᵂ$$
    
    
- 각 마스크는 해당 이미지 내의 "하나의 의미 있는 구성 요소(컴포넌트)"를 나타낸다.

정리하면

| 이름 | 기능 | 목적 |
| --- | --- | --- |
| **Recognize Anything** | 이미지 내 객체 자동 인식 | 다양한 물체 상황에 대응 가능하게 |
| **Grounded SAM** | 세그멘테이션 마스크 생성 | 사전학습 모델의 강력한 표현력 활용 |
| **클러스터링** | 마스크 정제 및 보완 | SAM 마스크의 불균형/불완전성 극복 |
| **IoU 기반 정합** | 컴포넌트 간 연결성 확보 | 구조적으로 일관된 마스크 생성 |

## Component-Aware Patch Matching (CAPM)

![method.png](/static/images/univad-ad/method.png)

기존의 단순 patch-level feature matching을 개선하여,

컴포넌트 단위 매칭, 배경 제거, 텍스트 기반 의미 비교를 통합하여 **정확한 이상 탐지(특히 구조적 이상)을 잘 하도록 만들어주는 모듈이다.** 

가장 먼저, 쿼리 이미지와 정상(reference) 이미지를 사전학습된 이미지 인코더(예: ResNet, ViT 등)를 통해 처리하여 feature map을 추출한다. 이 과정에서 쿼리 이미지에 대해서는 $F_q \in \mathbb{R}^{H_1 \times W_1 \times C}$, 그리고 K개의 정상 이미지에 대해서는 $F_n \in \mathbb{R}^{K \times H_1 \times W_1 \times C}$ 형태의 피처맵이 생성된다.

이후에는 특징 맵을 일정한 크기로 interpolation하여, 패치 단위의 특징을 갖는 $P_q \in \mathbb{R}^{H_2 \times W_2 \times C}, P_n \in \mathbb{R}^{K \times H_2 \times W_2 \times C}$형태의 **patch-level feature**로 변환한다. 이렇게 얻어진 패치들 간의 유사도를 바탕으로, 쿼리 이미지의 각 패치  $P_i^q$와 정상 이미지들의 모든 패치 $P_n$ 사이의 cosine 거리 중 **최소값**을 취해 해당 패치의 anomaly score로 사용한다. 
이 방식은 아래와 같이 작성할 수 있다.

$$
\text{Score}_{\text{pm}}(P_i^q) = \min(\text{distance}(P_i^q, P_n))
$$

이와 같은 기존의 patch feature matching은 빠르고 직관적이지만 limitation 이 존재한다고 지적한다. 예를 들어, 이미지의 **배경 영역과 전경(관심 객체) 영역을 구분하지 못하기 때문에**, 배경에서의 패턴 변화에도 anomaly score가 높게 나오는 **false positive**가 발생할 수 있다. 

또한, 서로 다른 컴포넌트(예: 다른 부품) 사이에서 색상이나 질감이 비슷한 경우, 전혀 상관없는 부분끼리 잘못 매칭되는 문제가 생길 수도 있다.

이러한 문제를 해결하기 위해 UniVAD는 component-aware 방식을 제안한다. 위에서 제안한 C3 모듈을 통해 얻은 **컴포넌트 마스크**를 활용하여, 이미지의 각 패치를 **컴포넌트 단위로 분리**한다. 즉, 각 정상 이미지의 패치 feature $P_n$에서, 특정 컴포넌트 마스크 $M_i$에 해당하는 위치의 패치들만 모아 하나의 집합 $P_n^i$ 를 구성한다. 쿼리 이미지의 패치 $P_q$도 동일한 방식으로 컴포넌트별로 나누어 $P_q^i$를 생성한다.

이제 쿼리 이미지의 각 컴포넌트 내 패치 $P_j^{\text{qi}}$는, 해당 컴포넌트에 해당하는 정상 패치 집합 $P_n^i$ 와만 비교된다. 그리고 이들 사이의 cosine 거리 중 최소값을 이상 점수로 사용하게 되며, 이는 아래와 같이 수식으로 표현된다:

$$
\text{Score}_{aware}(P_j^{\text{qi}}) = \min(\text{distance}(P_j^{\text{qi}}, P_n^i))
$$

이 방식은 **서로 다른 컴포넌트 간의 매칭을 방지**하고, **정확한 의미 단위 내에서의 비교**를 가능하게 하여 탐지 정확도를 크게 높일 수 있다.

이와 함께, UniVAD는 멀티모달 특징을 활용하는 **이미지-텍스트 기반의 비교 방식**을 추가하는데, 이는 단순히 색상이나 질감만으로는 파악하기 어려운 이상 상황을 감지하기 위해서 도입되었다. 먼저, "정상 상태"와 "이상 상태"를 설명하는 텍스트 문장을 준비하고, 이를 사전 학습된 텍스트 인코더(CLP, BERT 등)를 통해 임베딩한다. 
예를 들어:

- 정상을 나타내는 텍스트: `a normal machine part` → $T_n \in \mathbb{R}^C$
- 이상을 나타내는 텍스트: `a broken or missing component` → $T_a \in \mathbb{R}^C$

그 다음, 쿼리 이미지의 각 패치 특징 $P_j^{\text{qi}}$ 와 이 두 텍스트 임베딩 간의 **cosine similarity**를 구하고, softmax를 적용하여 각 패치가 정상이 더 가까운지, 이상이 더 가까운지를 나타내는 **확률적 score**를 생성한다:

$$
\text{Score}_{vl}(P_j^{\text{qi}}) = \text{softmax}(\text{sim}(P_j^{\text{qi}}, [T_n, T_a]))
$$

이렇게 하면 해당 패치가 **개념적으로 이상한지 여부**도 반영할 수 있게 되어, 의미 기반 탐지 정확도가 향상된다.

마지막으로, 위에서 설명한 세 가지 점수 —

(1) 기존 patch matching 점수 $\text{Score}_{\text{pm}},$

(2) 컴포넌트-aware 점수 $\text{Score}_{aware},$

(3) 이미지-텍스트 점수 $\text{Score}_{vl}$ —

를 가중합하여 최종적인 구조적 이상 점수 맵 $\text{Score}_{stru}$를 계산한다:

$$
\text{Score}_{stru} = \alpha \cdot \text{Score}_{\text{pm}} + \beta \cdot \text{Score}_{aware} + \gamma \cdot \text{Score}_{vl}
$$

실험에서는 각 가중치 $\alpha, \beta, \gamma$ 를 동일하게 1/3으로 설정하여 평균을 사용했다. 이 최종 구조적 이상 점수는 이후의 GECM에서 생성된 논리적 이상 점수와 결합되어 UniVAD의 최종 anomaly map으로 까지 연결된다.

---

## 기본 Patch Feature Matching 방식

### ① 이미지 인코더를 통한 피처 추출

- 사전 학습된 이미지 인코더 사용 (예: ResNet, ViT)
- 생성된 특징맵:
    - **Fq ∈ ℝᴴ₁×ᵂ₁×C**: 쿼리 이미지
    - **Fn ∈ ℝᴷ×ᴴ₁×ᵂ₁×C**: K개의 정상 이미지

### ② Interpolation을 통해 Patch Feature 생성

- Fq, Fn을 고정된 크기로 보간하여 패치 단위 특징 생성:
    - **Pq ∈ ℝᴴ₂×ᵂ₂×C**
    - **Pn ∈ ℝᴷ×ᴴ₂×ᵂ₂×C**

### ③ 기본 이상 점수 계산 (기존 방식)

- 쿼리 이미지의 각 패치 $P_i^q$ 에 대해 정상 이미지 전체 패치 $P_n$과의 **cosine distance** 최소값을 구함:
    
    $$
    \text{Score}_{\text{pm}}(P_i^q) = \min(\text{distance}(P_i^q, P_n))
    $$
    
- 근데 여기서의 문제점은 배경과 전경을 구분 못 함 → **배경에 false positive 발생**
- 다른 컴포넌트끼리 매칭될 수도 있음 → **유사 색이나 질감의 오류**

---

## Component-aware 개선: 컴포넌트 마스크 활용

이를 해결하기 위해, **C3 모듈의 component mask**를 활용하여 패치를 **컴포넌트 단위로 분리**해서 matching을 수행한다. 

---

### ① Pn에서 컴포넌트별 patch subset 생성

- 컴포넌트 마스크 $M_i \in ℝ^{H×W}$ 를 사용해
    
    각 컴포넌트 i에 해당하는 정상 이미지 패치들을 뽑아냄:
    
    $$
    P_n^i = \left\{ P_j^n \;\middle|\; M_i^j = 1 \right\}
    $$
    

### ② 쿼리 이미지도 동일 방식으로 분할 → $P_q^i$

### ③ 컴포넌트별 patch matching 수행

- 쿼리 이미지의 각 컴포넌트 내부에서만 patch를 비교
- 각 patch의 anomaly score는 해당 컴포넌트 내 정상 패치와의 거리 최소값:
    
    $$
    \text{Score}_{aware}(P_j^{\text{qi}}) = \min(\text{distance}(P_j^{\text{qi}}, P_n^i))
    $$
    
- 서로 **다른 컴포넌트**끼리 매칭되는 현상을 방지
- 의미적으로 더 **정확한** 비교 수행

---

## Image-Text 기반 anomaly score 추가

여기서 UniVAD의 핵심 강점인 **멀티모달 텍스트 정보**를 추가한다.

### 정상/이상 텍스트 피처 $T_n, T_a$ 생성

- 사전 학습된 **텍스트 인코더** 사용 (예: CLIP, BERT)
- 텍스트 예시:
    - 정상 텍스트: `a normal machine part` → $T_n ∈ ℝ^C$
    - 이상 텍스트: `a broken or missing component` → $T_a ∈ ℝ^C$

---

### 이미지 patch 피처와 텍스트 피처 간 cosine similarity 계산

- 각 쿼리 패치 특징 $P_j^{\text{qi}}$에 대해 $T_n, T_a$ 와의 유사도:
    
    $$
    \text{Score}_{vl}(P_j^{\text{qi}}) = \text{softmax}(\text{sim}(P_j^{\text{qi}}, [T_n, T_a]))
    $$
    
- softmax는 유사도를 확률처럼 정규화하여
    
    "이상이 더 가까운지, 정상이 더 가까운지" 판단
    
- 색상, 모양만 보던 기존 방식과 달리, **개념적으로 이상인지**를 판단할 수 있음

---

## 최종 Structural Anomaly Score 계산

위의 세 가지 score를 결합해서 최종 structural anomaly map을 생성한다:

$$
\text{Score}_{stru} = \alpha \cdot \text{Score}_{\text{pm}} + \beta \cdot \text{Score}_{aware} + \gamma \cdot \text{Score}_{vl}
$$

- 실험에서는 $\alpha = \beta = \gamma = \frac{1}{3}$로 설정
- 각 패치에 대해 **종합적인 이상도 평가**

---

| 요소 | 설명 | 장점 |
| --- | --- | --- |
| **Patch Matching (기본)** | 쿼리 패치와 정상 패치 간 거리 최소값 | 정밀 비교 가능하지만, 배경/구성 요소 구분 안 됨 |
| **Component-aware Matching** | 컴포넌트 단위로만 패치 매칭 | 오탐 제거, 구조 인식 강화 |
| **Image-Text Matching** | 패치와 “정상/이상” 텍스트 간 유사도 비교 | 의미 기반 이상 감지 가능 |
| **최종 Score** | 세 가지 점수의 가중합 | 시각+의미적 이상을 통합적으로 탐지 |

## Graph-Enhanced Component Modeling (GECM)

단순한 픽셀-level 또는 패치-level 비교로는 탐지하기 어려운, 논리적 이상 (logical anomalies) →  즉, **컴포넌트의 잘못된 조합/배치/누락**을 탐지하기 위한 AD모듈을 추가적으로 달았음

CAPM 한계를 보완하기 위해 넣었는데, 

| 방식 | 감지 가능한 이상 유형 | 한계 |
| --- | --- | --- |
| **CAPM** | 새로운 구조나 질감의 이상 (structural anomaly) | 기존 구성 요소지만 **배치가 이상**한 경우 탐지 어려움 |
| **GECM** | 구성 요소 자체의 의미와 관계성을 분석하여 논리적 이상 탐지 | 예: 부품이 **빠졌거나**, **중복되었거나**, **엉뚱한 위치에 있는 경우** |

이 모듈은 기존의 구조적 이상 탐지 방식(CAPM)만으로는 포착하기 어려운 logical anomalies까지 탐지할 수 있도록 하기 위해 GECM을 도입한 것이다. 여기서 말하는 논리적 이상이란, 이미지에 등장하는 구성 요소 자체는 정상 이미지들에도 존재하지만, 그것들이 **잘못된 방식으로 배치**되었거나, **중복되었거나**, 혹은 **누락된 경우**를 의미한다.

예를 들어, 정상적인 조립 기계에는 반드시 있어야 할 부품이 누락되었거나, 반대로 하나의 부품이 두 개 반복되어 있거나, 위치가 어긋나 있는 상황 등이 이에 해당된다. 이러한 이상은 단순히 패치 간의 유사도만으로는 감지하기 어렵기 때문에, UniVAD는 이미지의 전체 구조와 구성 요소 간의 관계를 고려하는 **그래프 기반 방식을 활용한 것이다.** 

---

### 입력 특징 추출 (컴포넌트 단위)

먼저, 쿼리 이미지와 정상 이미지에 대해 사전 학습된 이미지 인코더(예: ViT, ResNet 등)를 사용해 feature map을 추출한다. 쿼리 이미지는 $F_q \in \mathbb{R}^{H_1 \times W_1 \times C}$, 정상 이미지는 $F_n \in \mathbb{R}^{K \times H_1 \times W_1 \times C}$ 형태의 특징 맵으로 표현된다.

그 다음, 앞의 모듈 Contextual Component Clustering(C3) 에서 생성된 **컴포넌트 마스크**를 활용하여 각 컴포넌트 영역의 특징을 평균내는 **group average pooling**을 수행한다. 이 과정을 통해 쿼리 이미지의 경우 $F_{qc} \in \mathbb{R}^{N_q \times C}$, 정상 이미지의 경우 $F_{nc} \in \mathbb{R}^{K \times N_n \times C}$ 형태로 **컴포넌트 단위의 특징 벡터**가 생성된다.

---

### component를 그래프 구조로 모델링

이제 각 컴포넌트는 하나의 노드(node)로 간주되고, 이들 사이의 유사도는 그래프의 엣지(edge)로 표현된다. 즉, UniVAD는 **컴포넌트들 간의 의미적 관계**를 그래프 구조로 해석한다.

이 관계를 수치적으로 표현하기 위해, 각 컴포넌트 벡터 쌍 사이의 cosine 유사도를 계산하고, 그 결과로 **인접 행렬(adjacency matrix)** $A \in \mathbb{R}^{N \times N}$ 을 구성한다. 이때 유사도는 정규화를 위해 softmax 형태로 처리되며, 수식으로는 아래와 같이 표현된다.

$$
S_{ij} = \frac{S'_{ij}}{\sum_{k=1}^N S'_{ik}}, \quad S'_{ij} = \text{sim}(node_i, node_j)
$$

이 행렬은 쿼리 이미지와 정상 이미지 각각에 대해 독립적으로 생성되며, 컴포넌트 간의 관계(크기)를 나타낸다.

---

### Graph Attention을 통한 정보를 모으자!

생성된 인접 행렬과 컴포넌트 피처를 기반으로, UniVAD는 **Graph Attention Network (GAT)** 연산을 수행한다. 이 연산은 각 컴포넌트가 인접한 다른 컴포넌트들로부터 의미 정보를 집약하는 과정을 통해, 더욱 풍부하고 문맥적인 feature 정보들을 모으기 위해서다. 이를 통해 얻어진 그래프 임베딩은 아래와 같이 표현한다.

$$
E_q = G(A_q, F_{qc}), \quad E_n = G(A_n, F_{nc})
$$

여기서 G는 GAT 연산자로, 각 노드는 자신의 이웃 노드로부터 정보를 집약하여 고차원적 의미 벡터로 변환된다.

---

### Deep Feature 기반 이상 점수 계산

이렇게 생성된 쿼리 이미지의 컴포넌트 임베딩 E_q의 각 벡터 $E_i^q$에 대해, 모든 정상 이미지의 컴포넌트 임베딩 E_n과의 cosine 거리를 계산하고, 그 중 최소값을 해당 컴포넌트의 **deep feature 기반 이상 점수**로 사용한다.

$$
\text{Score}_{deep}(E_i^q) = \min(\text{distance}(E_i^q, E_n))
$$

즉, 정상 컴포넌트들과 충분히 유사한 임베딩이 존재하지 않는다면, 해당 쿼리 컴포넌트는 논리적으로 이상할 가능성이 높다고 판단하는 방식 !

---

### Geometric Feature 기반 보조 분석

하지만 deep feature만으로는 위치나 형태 같은 시각적인 배치를 완전히 파악하기 어려울 수 있기 때문에, UniVAD는 기하학적 특성(geometric features)도 함께 고려한다. 각 컴포넌트에 대해 다음과 같은 속성을 벡터로 추출한다.

- 컴포넌트의 면적 (마스크 내 픽셀 수)
- 평균 색상 (RGB)
- 중심 좌표 (x, y)

이 정보들을 결합하여, 쿼리 이미지에 대해  $G_q​∈ \mathbb{R}^{N_q \times C_g}$, 정상 이미지에 대해 $G_n \in \mathbb{R}^{N_n \times C_g}$ 형태의 **geometric feature vector**를 구성한다.

이후, 각 쿼리 컴포넌트 $G_i^q$에 대해 정상 컴포넌트들과의 거리 중 최소값을 **geometric anomaly score**로 다음과 같이 정의한다. 

$$
\text{Score}_{geo}(G_i^q) = \min(\text{distance}(G_i^q, G_n))
$$

예를 들어, 동일한 부품이라도 이미지에서 너무 다른 위치에 있거나 크기가 극단적으로 다르다면, 이는 논리적 이상으로 판단될 수 있다는 것.

---

### 최종 Logical Anomaly Score

위에서 계산한 deep feature 기반 이상 점수와 geometric feature 기반 이상 점수를 **가중 평균**하여 최종 논리적 이상 점수(logical anomaly score)를 계산한다:

$$
\text{Score}_{logic} = \varphi \cdot \text{Score}_{deep} + \psi \cdot \text{Score}_{geo}
$$

실험에서는 두 항목의 중요도를 동일하게 설정하여, $\varphi = \psi = 0.5$로 사용하였다.

---

### 구조적 이상과 논리적 이상 점수의 결합

마지막으로, 앞서 CAPM 모듈에서 계산된 **구조적 이상 점수** $\text{Score}_{stru}$와, 이번에 계산한 논리적 이상 점수$\text{Score}_{logic}$를 다시 가중 평균하여, 최종 anomaly map을 완성한다 : 

$$
\text{Score}_{final} = \delta \cdot \text{Score}_{stru} + \eta \cdot \text{Score}_{logic}
$$

이 역시 실험에서는 $\delta = \eta = 0.5$로 설정되어, 두 가지 이상 유형을 균등하게 반영하였다.

### 입력 피처 생성: 컴포넌트 단위 특징 벡터

**컴포넌트 마스크 활용 → Feature Map 생성**

- 이미지 인코더로부터 피처맵 추출:
    - **Fq ∈ ℝᴴ₁×ᵂ₁×C** (쿼리)
    - **Fn ∈ ℝᴷ×ᴴ₁×ᵂ₁×C** (정상 K장)

**Group Average Pooling → 컴포넌트 벡터 생성**

- 각 마스크 영역에 대한 평균 특징 추출
    - **Fqc ∈ ℝᴺq×C** (쿼리 이미지의 Nq개 컴포넌트)
    - **Fnc ∈ ℝᴷ×ᴺn×C** (정상 이미지들의 Nn개 컴포넌트)

### Graph 구조로 컴포넌트 관계 모델링

**[Component Feature Aggregator (CFA)]**

각 컴포넌트를 그래프의 노드(node)로 보고,

노드 간 유사도를 엣지(edge)로 모델링한다.

**수식 (8): 인접 행렬 생성**

$A = 
\begin{bmatrix}
S_{11} & S_{12} & \cdots & S_{1N} \\
S_{21} & S_{22} & \cdots & S_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
S_{N1} & S_{N2} & \cdots & S_{NN}
\end{bmatrix}$

- 각 $S_{ij}$: node i와 j 사이의 cosine similarity 기반 weight
- 이 유사도를 softmax-normalization:

$$
S_{ij} = \frac{S'_{ij}}{\sum_{k=1}^N S'_{ik}}, \quad S'_{ij} = \text{sim}(node_i, node_j)
$$ 

### Graph Attention 연산

- 인접 행렬 A와 컴포넌트 피처 Fqc, Fnc를 사용해
    
    **Graph Attention (GAT)** 연산 수행 → 더 풍부한 의미 임베딩 생성
    

$$
E_q = G(A_q, F_{qc}), \quad E_n = G(A_n, F_{nc})
$$

- G는 GAT 연산자로, 각 컴포넌트가 다른 노드로부터 의미를 집약하는 역할

### Deep Feature 기반 이상 점수 계산

- 쿼리 이미지의 각 컴포넌트 임베딩 $E_i^q$에 대해,
    
    모든 정상 임베딩 $E_n$과 거리 측정:
    

$$
\text{Score}_{deep}(E_i^q) = \min(\text{distance}(E_i^q, E_n))
$$ 

- 거리(예: cosine distance)가 크면 → 논리적으로 이상한 구성일 가능성

---

### Geometric Feature 기반 점수 추가

**단지 임베딩만으로는 부족할 수 있어서**, 컴포넌트의 **면적, 색상, 위치** 등 **기하학적 속성**도 고려한다.

각 컴포넌트에 대해:

- 면적 (마스크 내 픽셀 수)
- 색상 평균값 (RGB)
- 중심 좌표 등

→ 벡터로 결합:

- $Gq∈RNq×Cg, Gn∈RNn×Cg$

### Geometric anomaly score 계산:

$$
\text{Score}_{geo}(G_i^q) = \min(\text{distance}(G_i^q, G_n))
$$ 

- 예: 색은 같지만 위치가 완전히 다르면 이상

---

## 최종 Logical Anomaly Score

위 두 점수를 **가중 평균**하여 논리적 이상 점수로 통합:

$$
\text{Score}_{logic} = φ \cdot \text{Score}_{deep} + ψ \cdot \text{Score}_{geo}
$$

- 실험에서는 $φ = ψ = 0.5$

---

## 최종 Anomaly Score 통합

앞서 Section 3.3에서 구한 **구조적 이상 점수**

(= `Score_stru`)와 함께 최종 anomaly map을 만듭니다:

$$
\text{Score}_{final} = δ \cdot \text{Score}_{stru} + η \cdot \text{Score}_{logic}
$$

- 실험에서는 $δ = η = 0.5$

---

| 구성 | 역할 | 설명 |
| --- | --- | --- |
| **Feature Extractor + Mask** | 컴포넌트 피처 추출 | 이미지 인코더 + 컴포넌트 마스크로 vector 생성 |
| **Graph Attention** | 의미 관계 모델링 | 컴포넌트 간 유사성 학습 |
| **Scoredeep** | 깊은 의미의 논리 이상 탐지 | 임베딩 간 유사성으로 구조적 의미 분석 |
| **Scoregeo** | 기하 기반 이상 탐지 | 위치·면적·색상 기준 |
| **Scorelogic** | 논리 이상 점수 | deep + geo 기반 점수 통합 |
| **Scorefinal** | 최종 이상 점수 | 구조적 + 논리적 이상 결합 |

---

1. **C3 모듈**: 컴포넌트 단위로 이미지를 분할
2. **CAPM 모듈**: 각 패치의 이상 정도를 세 가지 방식으로 평가 (기존 매칭 + 컴포넌트 제한 + 텍스트)
3. **GECM 모듈**: 컴포넌트 간 관계와 속성으로 고차원적 논리 이상 탐지
4. 구조적 점수와 논리적 점수를 합쳐서 **최종 이상 탐지 결과** 생성


결과를 확인해보면 

![method.png](/static/images/univad-ad/method.png)

Unified된 모델임에도 불구하고 (심지어 none-training 방식) 성능이 어느정도 올라온 모습을 확인할 수 있다.
아직까지 데이터셋부터 좀 애매하다고 생각되는 logical anomalies가 존재하는 MVTec-LOCO에서는 성능이 좀 떨어지는 모습을 확인할 수 있다.
Unified + logical anomalies를 해결하기 위한 연구 방향도 좋을 것 같다.

이 논문에 대한 더 자세한 내용은, 본 논문을 참고하면 된다 !:  
[📄 Read the paper on arXiv](https://openaccess.thecvf.com/content/CVPR2025/html/Gu_UniVAD_A_Training-free_Unified_Model_for_Few-shot_Visual_Anomaly_Detection_CVPR_2025_paper.html)